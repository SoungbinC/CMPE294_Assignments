# **Training a GPT Model on *The Great Gatsby***  

This project demonstrates how to train a Transformer-based GPT model on F. Scott Fitzgerald's *The Great Gatsby*. The model generates text that stylistically mimics the original work.

---

## **Project Overview**  
Using PyTorch, this project involves:  
1. **Data Preparation**: Downloading, cleaning, and preprocessing *The Great Gatsby* text.  
2. **Model Design**: Implementing a GPT-like Transformer architecture.  
3. **Training**: Training the model on the cleaned text dataset.  
4. **Text Generation**: Generating new, stylistically consistent text using the trained model.

---

## **Artifacts**  
- **Medium Article**: [Training a GPT Model on The Great Gatsby](https://medium.com/@soungbin.cho/training-a-gpt-model-on-the-great-gatsby-1278dbf4db01)  
- **Google Colab Notebook**: [View and Run the Code](https://colab.research.google.com/drive/1pstxQ_oQNI7UXo2XB8KISpC0ttYcJC42#scrollTo=x_7_lqc33Ccr)  

---

## **Demo Video**  
ðŸŽ¥ Add your demo video here once it's available: 

https://github.com/user-attachments/assets/778789ae-3acb-40b6-9bb7-d91515ffa6a1


---

## **Usage**  
1. Clone the repository or use the Colab notebook to run the project.  
2. Replace the dataset with any plain text for experimenting with different styles or authors.  
3. Run the training script and generate text outputs.  

---

## **Results**  
Sample generated text from the model:  
> *"In the early twilight of autumn, the faint glimmer of Gatsby's dreams began to wane, scattering into the misty reaches of memory."*

---

## **Future Work**  
- Experiment with larger datasets for richer text generation.  
- Extend the model to generate text with specific prompts.  
- Explore fine-tuning pre-trained models like GPT-2 or GPT-3.  

---
``
